{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielcassimiro17/build-with-ai/blob/main/Build_With_AI_Gemini_GDG_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Setup Gemini no Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 1 criar uma conta na Google AI Studio"
      ],
      "metadata": {
        "id": "QEoTYxIaOmSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<a class=\"button button-primary\" href=\"https://aistudio.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Google AI Studio</a>\n",
        "\n"
      ],
      "metadata": {
        "id": "lIAGxMydKCJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 2 configurar API Key"
      ],
      "metadata": {
        "id": "oWMywhBmOqmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Antes de poder usar a API da Gemini, voc√™ deve obter uma chave de API. Se voc√™ ainda n√£o tem uma, crie uma chave com um clique no Google AI Studio.\n",
        "\n",
        "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Obeter uma chave de API</a>\n",
        "\n"
      ],
      "metadata": {
        "id": "JhdD953QJ_PF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 3: Setup API Key no notebook"
      ],
      "metadata": {
        "id": "w_HXRX0TOuSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Colab, adicione a chave ao gerenciador de segredos sob o √≠cone \"üîë\" no painel esquerdo. D√™ a ela o nome GOOGLE_API_KEY."
      ],
      "metadata": {
        "id": "fjIY7gT1KO-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini Basics"
      ],
      "metadata": {
        "id": "ig15oNhxdkHV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFPBKLapSCkM"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNV1e3ASJha"
      },
      "source": [
        "### Instalando o SDK Python\n",
        "\n",
        "\n",
        "O SDK do Python para a API Gemini est√° contido no pacote [`google-generativeai`](https://pypi.org/project/google-generativeai/). Instale a depend√™ncia usando pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OEoeosRTv-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a87c488-d43d-42bd-e652-1ee6a3855f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m146.8/146.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.5/664.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCFF5VSTbcAR"
      },
      "source": [
        "### Importando libs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRC2HngneEeQ"
      },
      "source": [
        "Importando as libs necess√°rias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('‚Ä¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d10c38a5c91f"
      },
      "outputs": [],
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHYFrFPjSGNq"
      },
      "source": [
        "### Configure sua chave de API no Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHhsUxDTdw0W"
      },
      "source": [
        "Dando acesso ao Colab para sua chave de API e acessando ela via c√≥digo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab9ASynfcIZn"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ssbTMNVSMd-"
      },
      "source": [
        "## Listar modelos\n",
        "\n",
        "Agora voc√™ est√° pronto para chamar a API Gemini. Use `list_models` para ver os modelos Gemini dispon√≠veis:\n",
        "\n",
        "* `gemini-pro`: otimizado para prompts somente de texto.\n",
        "* `gemini-pro-vision`: otimizado para prompts de texto e imagens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvvWFy08e5c5",
        "outputId": "54d419b2-cd3f-4b1d-966f-83ef872a5368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTl5NjtrhA0J"
      },
      "source": [
        "Nota: Para informa√ß√µes detalhadas sobre os modelos dispon√≠veis, incluindo suas capacidades e limites de taxa, consulte [Modelos Gemini](https://ai.google.dev/models/gemini). Existem op√ß√µes para solicitar [aumentos no limite de taxa](https://ai.google.dev/docs/increase_quota). O limite de taxa para os modelos Gemini-Pro √© de 60 solicita√ß√µes por minuto (RPM).\n",
        "\n",
        "O pacote `genai` tamb√©m suporta a fam√≠lia de modelos PaLM, mas somente os modelos Gemini suportam as capacidades gen√©ricas e multimodais do m√©todo `generateContent`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZfoK3I3hu6V"
      },
      "source": [
        "## Gerar texto a partir de entradas de texto\n",
        "\n",
        "Para prompts somente com texto, use o modelo `gemini-pro`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bcfnGEviwTI"
      },
      "outputs": [],
      "source": [
        "### Declarando o modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR_2A_sxk8sK"
      },
      "source": [
        "O m√©todo `generate_content` pode lidar com uma ampla variedade de casos de uso, incluindo conversas de m√∫ltiplas intera√ß√µes e entrada multimodal, dependendo do que o modelo subjacente suporta. Os modelos dispon√≠veis suportam apenas texto e imagens como entrada, e texto como sa√≠da.\n",
        "\n",
        "No caso mais simples, voc√™ pode passar uma string de prompt para o m√©todo <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content</code></a>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he-OfzBbhACQ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "#### usando o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbrR-n_qlpFd"
      },
      "source": [
        "Em casos simples, o acessor `response.text` √© suficiente. Para exibir texto Markdown formatado, use a fun√ß√£o `to_markdown`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-zBkueElVEO"
      },
      "outputs": [],
      "source": [
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZPpoKMQoru8"
      },
      "source": [
        "Se a API n√£o retornou um resultado, use `GenerateContentResponse.prompt_feedback` para verificar se o prompt foi bloqueado devido a quest√µes de seguran√ßa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIQdU8AGoraT"
      },
      "outputs": [],
      "source": [
        "#### Prompt feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEJupEDUo6Xj"
      },
      "source": [
        "O Gemini pode gerar v√°rias respostas poss√≠veis para um √∫nico prompt\n",
        "\n",
        "Essas respostas poss√≠veis s√£o chamadas de `candidates` (candidatos) e voc√™ pode analis√°-las para selecionar a mais adequada como resposta final.\n",
        "\n",
        "Visualize os candidatos de resposta com `GenerateContentResponse.candidates`: Documenta√ß√£o da API: [https://ai.google.dev/api/python/google/ai/generativelanguage/GenerateContentResponse#candidates](https://ai.google.dev/api/python/google/ai/generativelanguage/GenerateContentResponse#candidates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoGYz-I7o5wF"
      },
      "outputs": [],
      "source": [
        "response.candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJrwllLnHlBb"
      },
      "source": [
        "Por padr√£o, o modelo retorna uma resposta ap√≥s concluir todo o processo de gera√ß√£o. Voc√™ tamb√©m pode transmitir a resposta √† medida que ela √© gerada, e o modelo retornar√° partes da resposta assim que forem geradas.\n",
        "\n",
        "Para transmitir respostas, use `GenerativeModel.generate_content(..., stream=True)` Documenta√ß√£o da API: [https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7n59b3hHo6-"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "### stream\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jt0d0GCIUhg"
      },
      "outputs": [],
      "source": [
        "for chunk in response:\n",
        "  print(chunk.text)\n",
        "  print(\"_\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCzr5ZpNhxLm"
      },
      "source": [
        "## Gere texto a partir de entradas de imagem e texto\n",
        "\n",
        "Gemini oferece um modelo multimodal (`gemini-pro-vision`) que aceita tanto texto quanto imagens como entradas. A API `GenerativeModel.generate_content` √© projetada para lidar com prompts multimodais e retorna uma sa√≠da de texto.\n",
        "\n",
        "Vamos incluir uma imagem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtNGTBFF8Pgl"
      },
      "outputs": [],
      "source": [
        "!curl -o image.jpg https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcQ_Kevbk21QBRy-PgB4kQpS79brbmmEG7m3VOTShAn4PecDU5H5UxrJxE3Dw1JiaG17V88QIol19-3TM2wCHw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjnS0vNTsVis"
      },
      "outputs": [],
      "source": [
        "import PIL.Image\n",
        "\n",
        "img = PIL.Image.open('image.jpg')\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r99TN2R8EUD"
      },
      "source": [
        "Use o modelo `gemini-pro-vision` e passe a imagem para o modelo com `generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtXxgVzmJZzE"
      },
      "outputs": [],
      "source": [
        "#### declarando o modelo vision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwYifv298Cj3"
      },
      "outputs": [],
      "source": [
        "### generate content\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(response.text)"
      ],
      "metadata": {
        "id": "CRyI-u7K-nDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xW2Kyra8pSz"
      },
      "source": [
        "Para fornecer tanto texto quanto imagens em um prompt, passe uma lista contendo as strings e imagens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm9tUYeT8lBc"
      },
      "outputs": [],
      "source": [
        "#### Prompt e imagem\n",
        "\n",
        "prompt = \"\"\"Escreva a receita para o prato presente na imagem, em portug√™s.\"\"\"\n",
        "\n",
        "response = model.generate_content([prompt, img], stream=True)\n",
        "response.resolve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d46826OA9IDS"
      },
      "outputs": [],
      "source": [
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEgVOYu0pAr4"
      },
      "source": [
        "## Contar tokens\n",
        "\n",
        "Modelos de linguagem grandes possuem uma janela de contexto, e o comprimento do contexto √© frequentemente medido em termos do **n√∫mero de tokens**. Com a API Gemini, voc√™ pode determinar o n√∫mero de tokens para qualquer objeto `glm.Content`. No caso mais simples, voc√™ pode passar uma string de consulta para o m√©todo `GenerativeModel.count_tokens` da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLjBmPCLpElk"
      },
      "outputs": [],
      "source": [
        "#### count tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM2_U8pmpHQA"
      },
      "source": [
        "Da mesma forma, voc√™ pode verificar `token_count` para a resposta do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0MUU4BZpG4_"
      },
      "outputs": [],
      "source": [
        "#### count tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain"
      ],
      "metadata": {
        "id": "CRHIN4fWM4GC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O que √© LangChain\n",
        "\n",
        "LangChain √© um framework para o desenvolvimento de aplica√ß√µes potencializadas por LLMs.\n",
        "\n",
        "LangChain simplifica cada est√°gio do ciclo de vida da aplica√ß√£o LLM:\n",
        "\n",
        "- Desenvolvimento: Construa suas aplica√ß√µes usando os blocos de constru√ß√£o e componentes de c√≥digo aberto do LangChain. Comece rapidamente usando integra√ß√µes de terceiros e Templates.\n",
        "- Produ√ß√£o: Use o LangSmith para inspecionar, monitorar e avaliar suas cadeias, de modo que voc√™ possa otimizar continuamente e implantar com confian√ßa.\n",
        "- Implanta√ß√£o: Transforme qualquer cadeia em uma API com o LangServe."
      ],
      "metadata": {
        "id": "WGov0rjHNZL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vantagens\n",
        "\n",
        "- LangChain permite conectar LLMs a fontes de dados externas para aplicativos de NLP personalizados\n",
        "- Fornece m√≥dulos para integrar LLMs, fontes de dados e armazenamento\n",
        "- Possibilita a constru√ß√£o de aplicativos de IA conversacional, sumariza√ß√£o, busca e outras capacidades de NLP\n",
        "- Ajuda os usu√°rios a aproveitar a NLP e os LLMs para v√°rias aplica√ß√µes espec√≠ficas da ind√∫stria de maneira eficiente e eficaz"
      ],
      "metadata": {
        "id": "44BsXcsVNtxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet  langchain-google-genai langchain pip install faiss-cpu"
      ],
      "metadata": {
        "id": "eQIH6-LAVMVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup de vari√°vel de ambiente para uso do LangChain:"
      ],
      "metadata": {
        "id": "y4UdlR3UOQNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] =userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "D0C1HZt2Vayf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chamada Simples"
      ],
      "metadata": {
        "id": "mp03tnqdM9Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "####"
      ],
      "metadata": {
        "id": "TRWHzVdzVrTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos usar streaming aqui tamb√©m, com cada bloco de tokens sendo retornado assim que for gerado.\n",
        "\n",
        "Dessa forma conseguimos dar uma resposta mais r√°pida ao usu√°rio que vai acompanhando a gera√ß√£o dos tokens em tempo real"
      ],
      "metadata": {
        "id": "vXC1Ln85ObdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Streaming com langchain\n",
        "\n"
      ],
      "metadata": {
        "id": "L8qdnhcFV4cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construindo um caso de resumo de textos"
      ],
      "metadata": {
        "id": "hARy_tntNBXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para o caso de resumo de um longo texto vamos usar o modelo ***Gemini 1.5 Pro***.\n",
        "\n",
        "Esse modelo √© capaz de processar at√© **1 Milh√£o de tokens** em um √∫nico prompt.\n",
        "\n",
        "\n",
        "Para isso vamos usar o livro **Dom Casmurro** como exemplo."
      ],
      "metadata": {
        "id": "fsSNZfcJOrjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baixando o livro via link direto"
      ],
      "metadata": {
        "id": "xucUGNkFO-T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL to a public domain text by Machado de Assis, for example, \"Dom Casmurro\"\n",
        "url = \"https://www.gutenberg.org/cache/epub/55752/pg55752.txt\"\n",
        "\n",
        "response = requests.get(url)\n",
        "livro = response.text\n",
        "\n",
        "# Display the first 1000 characters to verify\n",
        "print(livro[1500:3000])\n"
      ],
      "metadata": {
        "id": "xBmtCk-zZsHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostrando Tamanho da string contendo o livro"
      ],
      "metadata": {
        "id": "YOGwjbBzPFRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(livro))"
      ],
      "metadata": {
        "id": "ESzg4PnaZ6r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos tamb√©m usar a fun√ß√£o de contar tokens ***(count_tokens)*** para ver tamanho do livro que queremos processar."
      ],
      "metadata": {
        "id": "lua3UroRPJNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Gemini 1.5\n",
        "\n",
        "\n",
        "#### count tokens"
      ],
      "metadata": {
        "id": "8V7b4DfDZ_Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passando livro no prompt para resumo"
      ],
      "metadata": {
        "id": "9NUBUe_mPUBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "### Carregando gemini-1.5-pro-latest\n",
        "\n",
        "### definindo o prompt\n",
        "\n",
        "#### chamando LLM\n",
        "\n",
        "#### mostrando resultado\n"
      ],
      "metadata": {
        "id": "k6jGkdsEXWiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando dados no prompt"
      ],
      "metadata": {
        "id": "EIsnudygNEQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos tamb√©m fazer **perguntas sobre o texto de contexto** que estamos passando para LLM. Dessa forma a **resposta ser√° baseada nesse contexto** passado.\n",
        "\n",
        "Essa aplica√ß√£o permite **mostrar para a LLM dados n√£o vistos no seu treinamento**, fornecendo assim contexto para que ela responda perguntas desses temas.\n"
      ],
      "metadata": {
        "id": "0rMwoixxPiXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pergunta= \"\"\"\n",
        "Em 'Dom Casmurro', Bentinho descreve um h√°bito peculiar de Capitu que ele observa repetidamente.\n",
        "Qual √© esse h√°bito e como ele interpreta essa caracter√≠stica dela?\n",
        "\"\"\"\n",
        "\n",
        "### Formatar prompt"
      ],
      "metadata": {
        "id": "yQqtIdzCb5FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Invoke LLM\n",
        "\n",
        "\n",
        "### mostrar resultado"
      ],
      "metadata": {
        "id": "pXckZOtxQChD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por√©m nem todas as LLMs suportam uma quantidade t√£o grande de tokens e quanto mais tokens passados em um prompt mais caro ser√° a chamada.\n",
        "\n",
        "Quando falamos de contexto de LLMs sempre temos que considerar o tradeoff:"
      ],
      "metadata": {
        "id": "aYU2HSVzQG6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Qualidade/Capacidade\n",
        "- Custo\n",
        "- Tempo de resposta  \n",
        "\n",
        "\n",
        "\n",
        "LLMs mais \"potentes\" tendem a ter um custo mais por token e demorar mais a responder. Por isso √© necess√°rio utilizar LLMs \"menos potentes\" sempre que poss√≠vel."
      ],
      "metadata": {
        "id": "PwOK54UfQckV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vlvtW9mbQ4MN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testando com o Gemini Pro 1.0:"
      ],
      "metadata": {
        "id": "zjDArbfIQ0Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pergunta= \"\"\"\n",
        "Em 'Dom Casmurro', Bentinho descreve um h√°bito peculiar de Capitu que ele observa repetidamente.\n",
        "Qual √© esse h√°bito e como ele interpreta essa caracter√≠stica dela?\n",
        "\"\"\"\n",
        "\n",
        "### Carregando gemini-pro\n",
        "\n",
        "### definindo o prompt\n",
        "\n",
        "#### chamando LLM\n",
        "\n",
        "#### mostrando resultado"
      ],
      "metadata": {
        "id": "cJPBSfSFcucr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### chamando direto da lib google"
      ],
      "metadata": {
        "id": "6KgdHFx8fjgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por esse motivo podemos usar a t√©cnica de RAG:"
      ],
      "metadata": {
        "id": "qRsb0M1JQ74E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9bU0J3vUIbz"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O conceito de embeddings √© fundamental para o funcionamento da t√©cnica de RAG (Retrieval-Augmented Generation), uma abordagem h√≠brida que combina a recupera√ß√£o de informa√ß√µes e a gera√ß√£o de texto."
      ],
      "metadata": {
        "id": "4dkFtecpRCqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Uso de Embeddings em RAG:** Os embeddings s√£o aplicados para representar e indexar grandes quantidades de dados textuais.\n",
        "- **Facilita√ß√£o da Busca:** Eles permitem que o modelo de RAG busque de forma eficiente os trechos mais relevantes de acordo com o contexto ou pergunta espec√≠fica."
      ],
      "metadata": {
        "id": "RB6sG5PsRZ2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cFJJeUqARfix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando Embeddings da Google"
      ],
      "metadata": {
        "id": "nqERhU4PRgZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "### inicializando embeddings\n",
        "\n",
        "### aplicando embeddings\n",
        "\n",
        "print(vector[:5])\n",
        "print(f\"Tamanho do vetor: {len(vector)}\")"
      ],
      "metadata": {
        "id": "eTwiZaRngRic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos demonstrar como RAG funciona com esse exemplo simples comparando 4 frases:"
      ],
      "metadata": {
        "id": "6rUsLmgaRoPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vector_1 = embeddings.embed_query(\"Eu amo Intelig√™ncia artificial\")\n",
        "vector_2 = embeddings.embed_query(\"Flores tropicais florescem predominantemente no ver√£o.\")\n",
        "vector_3 = embeddings.embed_query(\"Alan Turing desenvolveu o conceito de computa√ß√£o moderna\")\n",
        "vector_4 = embeddings.embed_query(\"Eu gosto muito de Intelig√™ncia artificial\")\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    dot_product = np.dot(v1, v2)\n",
        "    norm_v1 = np.linalg.norm(v1)\n",
        "    norm_v2 = np.linalg.norm(v2)\n",
        "    return dot_product / (norm_v1 * norm_v2)\n",
        "\n",
        "similarity_1_2 = cosine_similarity(vector_1, vector_2)\n",
        "\n",
        "similarity_1_3 = cosine_similarity(vector_1, vector_3)\n",
        "\n",
        "similarity_1_4 = cosine_similarity(vector_1, vector_4)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Cosine Similarity between vector 1 and vector 2: {similarity_1_2}\")\n",
        "print(f\"Cosine Similarity between vector 1 and vector 3: {similarity_1_3}\")\n",
        "print(f\"Cosine Similarity between vector 1 and vector 3: {similarity_1_4}\")\n"
      ],
      "metadata": {
        "id": "2PP2k-0MiTGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector DBs"
      ],
      "metadata": {
        "id": "U-vVme9eNN3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bancos de dados vetoriais s√£o cruciais para o RAG porque armazenam e gerenciam embeddings de forma eficiente. Esse processo de recupera√ß√£o √© essencial, pois enriquece a entrada para o modelo gerador, aumentando a precis√£o e a riqueza das respostas que ele pode produzir.\n",
        "\n",
        "- **Armazenamento em Bancos de Dados Vetoriais:** Os embeddings s√£o armazenados em bancos de dados vetoriais, otimizando a gest√£o de grandes volumes de dados.\n",
        "- **Recupera√ß√£o R√°pida de Informa√ß√µes:** Esses bancos permitem que o modelo RAG realize buscas r√°pidas baseadas em similaridade vetorial.\n"
      ],
      "metadata": {
        "id": "iyBXta-JR48Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**O chunking de documentos** √© o processo de dividir textos grandes em segmentos menores, chamados chunks, para facilitar o gerenciamento, aumentar a qualidade de recupera√ß√£o desses textos e permitir a redu√ß√£o da quantidade de tokens passados no prompt.\n",
        "\n",
        "Esses segmentos s√£o ent√£o inseridos em um banco de dados vetorial, permitindo que cada peda√ßo seja indexado e recuperado eficientemente com base em sua semelhan√ßa sem√¢ntica."
      ],
      "metadata": {
        "id": "xaG6uuLETXX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "### Definindo text_splitter\n",
        "\n",
        "### aplicando text splitter\n",
        "\n",
        "print(len(docs))\n",
        "print(len(docs[14].page_content))\n",
        "print(docs[10])\n",
        "print(docs[11])\n",
        "print(docs[12])"
      ],
      "metadata": {
        "id": "gg0SadsekPjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui vamos usar o Vector DB FAISS, que armazena os embeddings em mem√≥ria.\n",
        "\n",
        "Para experimenta√ß√£o r√°pida ele √© uma boa alternativa, mas para aplica√ß√µes em escala que voc√™ precisa persistir os dados, √© recomendado usar alguma vers√£o em cloud ou fazer o deploy do seu pr√≥prio."
      ],
      "metadata": {
        "id": "FQRgNFqMS72K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "### Vetorizando para o FAISS\n",
        "vector_store = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "retriever.search_kwargs[\"fetch_k\"] = 10\n",
        "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
        "retriever.search_kwargs[\"k\"] = 3\n",
        "\n",
        "\n",
        "question = \"Qual era a cor do vestido que Capitu usava?\"\n",
        "context = retriever.get_relevant_documents(question)\n",
        "context"
      ],
      "metadata": {
        "id": "jTlz3Es2lDcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG"
      ],
      "metadata": {
        "id": "GYLnEOMqNKeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n"
      ],
      "metadata": {
        "id": "EUeF2H3xmC-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Pergunta\n",
        "pergunta= \"Qual era a cor do vestido que Capitu usava?\"\n",
        "\n",
        "\n",
        "### Obter contexto\n",
        "\n",
        "\n",
        "### Formatar prompt\n",
        "prompt = f\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model.count_tokens(prompt)\n",
        "\n"
      ],
      "metadata": {
        "id": "C3XySBZ4ldAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(prompt)\n",
        "response.content"
      ],
      "metadata": {
        "id": "TxJOzM4XmUy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexto = retriever.get_relevant_documents(question)\n",
        "contexto"
      ],
      "metadata": {
        "id": "B3AL9h9-F19q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imagens com Langchain"
      ],
      "metadata": {
        "id": "EmT-szkaO4AN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos tamb√©m chamar os modelos multi-modais usando LangChain.\n",
        "\n",
        "\n",
        "Aqui podemos passar a imagem ou diretamente a url da imagem durante a chamada do modelo."
      ],
      "metadata": {
        "id": "pCnkmqTpU_DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "llm_vision = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
        "\n",
        "image_url = \"https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcQ_Kevbk21QBRy-PgB4kQpS79brbmmEG7m3VOTShAn4PecDU5H5UxrJxE3Dw1JiaG17V88QIol19-3TM2wCHw\"\n",
        "\n",
        "prompt =  \"Create an in depth description of the image\"\n",
        "\n",
        "product_msg = HumanMessage(\n",
        "    content=[\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": prompt,\n",
        "        },\n",
        "        {\"type\": \"image_url\", \"image_url\": image_url},\n",
        "    ]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "bn9MHzuApTqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prod_output = llm_vision.invoke([product_msg])\n",
        "\n",
        "print(prod_output.content)"
      ],
      "metadata": {
        "id": "oLY-F8m49R5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Chains"
      ],
      "metadata": {
        "id": "rn6rebhUO1Sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As \"chains\" do LangChain s√£o cadeias de componentes que combinam v√°rias habilidades e servi√ßos de LLMs para resolver problemas complexos de processamento de linguagem natural.\n",
        "\n",
        "A grande vantagem das chains do LangChain √© a **modularidade**, permitindo que desenvolvedores construam aplica√ß√µes customizadas conectando diferentes m√≥dulos, como recupera√ß√£o de informa√ß√µes, entendimento de texto e gera√ß√£o de linguagem.\n",
        "\n",
        "Isso resulta em solu√ß√µes mais robustas, flex√≠veis e escal√°veis, adapt√°veis a uma ampla gama de casos de uso espec√≠ficos da ind√∫stria.\n",
        "\n",
        "https://python.langchain.com/docs/expression_language/why/"
      ],
      "metadata": {
        "id": "_3Mf6AsbTquG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo B√°sico"
      ],
      "metadata": {
        "id": "ivD1W8-3UxDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "#### Create Chain\n",
        "\n",
        "\n",
        "\n",
        "### Invoke Chain\n",
        "\n"
      ],
      "metadata": {
        "id": "hYssRFFBhAOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Chain"
      ],
      "metadata": {
        "id": "5jdcr6k9Uy_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup de componentes:"
      ],
      "metadata": {
        "id": "Cut-FJpYU7gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "\n",
        "template = \"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "setup_and_retrieval = RunnableParallel(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        ")\n"
      ],
      "metadata": {
        "id": "HTpDvn7sWfyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constru√ß√£o da Chain"
      ],
      "metadata": {
        "id": "_5UPJUXJU9MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Chain\n",
        "\n",
        "\n",
        "chain.invoke(\"Qual a cor do olho de capitu?\")"
      ],
      "metadata": {
        "id": "pSFlZ_CqU6kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos tamb√©m chamar um componente diretamente"
      ],
      "metadata": {
        "id": "QZ0JY5c9VaMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Invoke retriever"
      ],
      "metadata": {
        "id": "NSR_4Q8FUBhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A fun√ß√£o Debug do LangChain permite que tenhamos visibilidade do que est√° acontecendo dentro das etapas da chain."
      ],
      "metadata": {
        "id": "IE5AKLU3VjN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "\n",
        "langchain.deubg = True"
      ],
      "metadata": {
        "id": "1LbWylZtVfK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Invoke"
      ],
      "metadata": {
        "id": "M494101aVh_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.deubg = False"
      ],
      "metadata": {
        "id": "cYc8PpIoVvso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Casos de Uso"
      ],
      "metadata": {
        "id": "HxONCuIyNNnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rag na Wikipedia"
      ],
      "metadata": {
        "id": "KhwhJKsah4e1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar um chatbot capaz de responder perguntas sobre o tema presente nessa p√°gina da Wikipedia:\n",
        "\n",
        "Deve responder perguntas como:\n",
        "\n",
        "\" Quais modelos do Gemini est√£o dispon√≠veis?\"  \n",
        "\" Quais modelos da Anthropic existem?\"  \n",
        "\" Quem √© o CEO do twitter?\"    \n",
        "\" O Twitter tem alguma llm?\"    "
      ],
      "metadata": {
        "id": "M_ma7JLEt6A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  wikipedia"
      ],
      "metadata": {
        "id": "wQgykReGt5MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "wikipedia.run(\"Sam Altman\")"
      ],
      "metadata": {
        "id": "2Kr2C5MbX5GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_personalities_companies = [\n",
        "    \"Sam Altman\",\n",
        "    \"Demis Hassabis\",\n",
        "    \"Geoffrey Hinton\",\n",
        "    \"Andrew Ng\",\n",
        "    \"Elon Musk\",\n",
        "    \"DeepMind\",\n",
        "    \"Meta AI\",\n",
        "    \"OpenAI\",\n",
        "    \"Google Brain\",\n",
        "    \"Google Gemini\",\n",
        "    \"Claude 3\",\n",
        "    \"Llama 3\",\n",
        "    \"GPT 4\",\n",
        "    \"Grok\"\n",
        "]\n",
        "\n",
        "# Initialize a list to store summaries\n",
        "summaries = []\n",
        "\n",
        "# Loop through the list and search for each entry\n",
        "for entry in ai_personalities_companies:\n",
        "    summary = wikipedia.run(entry)\n",
        "    summaries.append(summary)  # Append the result to the summaries list\n",
        "\n"
      ],
      "metadata": {
        "id": "kRCPRNg9X8_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries[0]"
      ],
      "metadata": {
        "id": "uPAdeJKAjer0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Desenvolva o c√≥digo aqui:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5Ix9FnZLV3cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cria√ß√£o de posts com base em fotos"
      ],
      "metadata": {
        "id": "dMXugzu9h84V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crie um post para uma rede social promovendo o prato que est√£o nas fotos"
      ],
      "metadata": {
        "id": "sL64Al8d08V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Function to download and display images\n",
        "def download_display_images(urls):\n",
        "    plt.figure(figsize=(15, 10))  # Set the figure size\n",
        "    for i, url in enumerate(urls):\n",
        "        try:\n",
        "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "            response = requests.get(url, headers=headers)\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            plt.subplot(1, len(urls), i + 1)  # Subplot for each image\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')  # Turn off axis numbers and ticks\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load image from {url}: {e}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def display_text_image(text, image_url):\n",
        "    \"\"\"\n",
        "    Displays text and an image side by side using matplotlib.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The text to display.\n",
        "    - image_url (str): The URL of the image to display.\n",
        "    \"\"\"\n",
        "    # Load the image from the URL\n",
        "    response = requests.get(image_url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "\n",
        "    # Set up the figure and axes\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "    # Display the image on the left\n",
        "    axs[0].imshow(img)\n",
        "    axs[0].axis('off')  # Turn off axis for image\n",
        "\n",
        "    # Display the text on the right\n",
        "    axs[1].text(0.5, 0.5, text, fontsize=12, ha='center', va='center', wrap=True)\n",
        "    axs[1].axis('off')  # Turn off axis for text\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "TxTwXmS4e8q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URLs of images\n",
        "image_urls = [\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/a/a3/Eq_it-na_pizza-margherita_sep2005_sml.jpg\",\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Egyptian_food_Koshary.jpg/640px-Egyptian_food_Koshary.jpg\",\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Tomato_je.jpg/640px-Tomato_je.jpg\",\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f9/My_food_platter.jpg/640px-My_food_platter.jpg\",\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Pizza_with_food_waste.jpg/640px-Pizza_with_food_waste.jpg\",\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Yakisoba_%28Japanese_style_chow_mein%29_%E7%84%BC%E3%81%8D%E3%81%9D%E3%81%B0.jpg/640px-Yakisoba_%28Japanese_style_chow_mein%29_%E7%84%BC%E3%81%8D%E3%81%9D%E3%81%B0.jpg\",\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Fast_Food_in_Lusaka_10.jpg/640px-Fast_Food_in_Lusaka_10.jpg\",\n",
        "]"
      ],
      "metadata": {
        "id": "xleur4dEcwdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function with the list of URLs\n",
        "download_display_images(image_urls)"
      ],
      "metadata": {
        "id": "ftRIo9LOcyLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "text = \"This is an example of displaying text and an image side by side.\"\n",
        "\n",
        "display_text_image(text, image_urls[0])"
      ],
      "metadata": {
        "id": "wfK0DsWZ_SAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Desenvolva o c√≥digo aqui:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "liTWSOhFUIlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recursos e pr√≥ximos passos"
      ],
      "metadata": {
        "id": "iN879efdILCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documenta√ß√µes:\n",
        "- https://ai.google.dev/gemini-api/docs/get-started/python\n",
        "- https://python.langchain.com/docs/expression_language/get_started/\n",
        "- https://python.langchain.com/docs/integrations/chat/google_generative_ai/\n",
        "- https://ai.google.dev/pricing?hl=pt-br\n",
        "\n",
        "Cursos:\n",
        "- https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/\n",
        "- https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/\n",
        "- https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/\n",
        "\n",
        "\n",
        "\n",
        "Criando um chatbot com interface simples:\n",
        "- https://blog.streamlit.io/how-to-build-an-llm-powered-chatbot-with-streamlit/"
      ],
      "metadata": {
        "id": "rclcrN3JINrP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}